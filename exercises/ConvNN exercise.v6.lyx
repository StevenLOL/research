#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble


\usepackage{amsfonts}

\usepackage{amsthm}\newtheorem{theorem}{Theorem}[section]\newtheorem{lemma}[theorem]{Lemma}\newtheorem{proposition}[theorem]{Proposition}\newtheorem{corollary}[theorem]{Corollary}

\title{Fine-tuningwithCDBN}\author{HonglakLee}

\newcommand{\R}{\mathbb{R}}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref false
\papersize letterpaper
\use_geometry true
\use_amsmath 2
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Backpropagation with Convolutional Neural Nets
\end_layout

\begin_layout Section
Intro to Convolution Operators
\end_layout

\begin_layout Standard
(Note: in this tutorial, we use dummy index variables 
\begin_inset Formula $i,j,l$
\end_inset

,etc.
 that start from 1, not 0.)
\end_layout

\begin_layout Paragraph
Filtering
\end_layout

\begin_layout Standard
Filtering between the two vector 
\begin_inset Formula $a,b$
\end_inset

 (here you can image 
\begin_inset Formula $a$
\end_inset

 as 1-d image, and 
\series bold

\begin_inset Formula $b$
\end_inset


\series default
 as 1-d filter.
 Then the filtering is defined as: 
\begin_inset Formula 
\begin{equation}
(a*_{filt}b)_{i}=\sum_{j}a_{i+j-1}b_{j}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Given 
\begin_inset Formula $a\in R^{N},b\in R^{M}$
\end_inset

, we have 
\begin_inset Formula $a*b\in R^{N-M+1}$
\end_inset

.
 (We assume 
\begin_inset Formula $N\ge M$
\end_inset

 here).
\end_layout

\begin_layout Paragraph

\series bold
Valid convolution
\end_layout

\begin_layout Standard
Given two vectors 
\begin_inset Formula $a\in R^{N},b\in R^{M}$
\end_inset

, the valid convolution can be thought as filtering after flipping the filter
 
\begin_inset Formula $b$
\end_inset

, which is formally defined as: 
\begin_inset Formula 
\begin{equation}
(a*_{valid}b)_{i}=\sum_{j}a_{j}b_{i-j+N}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Indeed, now we can show that 
\begin_inset Formula $(a*_{valid}\tilde{b})=(a*_{filt}b)$
\end_inset

, where 
\begin_inset Formula $\tilde{b}$
\end_inset

 is a flipped vector of 
\begin_inset Formula $b$
\end_inset

 (flipping left-right for 1d case, and it will be flipping left-right/up-down
 for 2d case).
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
(a*_{valid}\tilde{b})_{i}\triangleq\sum_{j}a_{j}\tilde{b}_{i-j+N}=\sum_{j}a_{j}b_{j-i+1}=\sum_{k}a_{i+k-1}b_{k}\triangleq(a*_{filt}b)_{i}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
In the derivation, we simply replaced the dummy variable 
\begin_inset Formula $k\triangleq j-i+1$
\end_inset

.
\end_layout

\begin_layout Subsubsection*
Full convolution
\end_layout

\begin_layout Standard
Full convolution between two vectors 
\begin_inset Formula $a\in R^{N},b\in R^{M}$
\end_inset

 is defined as: 
\begin_inset Formula 
\begin{equation}
(a*_{full}b)_{i}=\sum_{j}a_{j}b_{i-j+1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The output is a 
\begin_inset Formula $N+M-1$
\end_inset

 dimensional vector.
\end_layout

\begin_layout Section
Example
\end_layout

\begin_layout Standard
Suppose that we have convolutional neural network with one layer.
 We assume 1-dimensional case for simplicity.
 We are given input vector 
\begin_inset Formula $\mathbf{v}\in R^{N_{V}}$
\end_inset

, hidden units 
\begin_inset Formula $\mathbf{h}\in R^{N_{H}\times K}$
\end_inset

 and filter weights 
\series bold

\begin_inset Formula $\mathbf{w}\in R^{N_{W}\times K}$
\end_inset


\series default
 (there are 
\begin_inset Formula $K$
\end_inset

 filters and each filter has 
\begin_inset Formula $N_{W}$
\end_inset

 dimension).
 From the output of this convolutional network, we put logistic regression
 with parameter 
\begin_inset Formula $\theta$
\end_inset

 to predict the class label 
\begin_inset Formula $y\in\{0,1\}$
\end_inset

.
 Then the below is the relationship between input and output prediction:
 
\begin_inset Formula 
\begin{eqnarray*}
a_{j}^{k} & = & \left(\mathbf{v}*_{val}\tilde{\mathbf{w}}^{k}\right)_{j}+b^{k}=\sum_{l}v_{j+l-1}w_{l}^{k}+b^{k}\\
h_{j}^{k} & = & sigmoid(a_{j}^{k})\\
\hat{y} & = & sigmoid(\mathbf{\theta}^{T}\mathbf{h})=sigmoid(\sum_{kj}\theta_{kj}h_{j}^{k})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
here, 
\begin_inset Formula $b^{k}$
\end_inset

 is a bias for the hidden units.
 Roughly speaking, 
\begin_inset Formula $\mathbf{a}\in R^{N_{H}\times K}$
\end_inset

 (where 
\begin_inset Formula $N_{H}=N_{V}-N_{W}+1$
\end_inset

) is the linear filter response, 
\begin_inset Formula $\mathbf{h}\in R^{N_{H}\times K}$
\end_inset

 the first layer hidden unit activations, and 
\begin_inset Formula $\mathbf{y}$
\end_inset

 is the final prediction.
 We define the loss function as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f(y,\hat{y})=y\log\left(sigmoid(\theta^{T}\mathbf{h})\right)+(1-y)\log\left(1-sigmoid(\theta^{T}\mathbf{h})\right)
\]

\end_inset

Now we apply back-propagation to this loss function:
\end_layout

\begin_layout Paragraph
Exercise 1.
\end_layout

\begin_layout Standard
Derive the following:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial f}{\partial\theta_{kj}} & = & (y-\hat{y})h_{j}^{k}\\
\frac{\partial f}{\partial h_{j}^{k}} & = & (y-\hat{y})\theta_{kj}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard

\series bold
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout

\series bold
Solution:
\end_layout

\begin_layout Plain Layout
The first term:
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\frac{\partial f}{\partial\theta_{kj}}=y\frac{\partial}{\partial\theta_{kj}}log(sigmoid(\Sigma_{kj}\theta_{kj}h_{j}^{k}))+(1-y)\frac{\partial}{\partial\theta_{kj}}log(1-sigmoid(\Sigma_{kj}\theta_{kj}h_{j}^{k}))$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $=\frac{y}{\hat{y}}(1-\hat{y})\hat{y}\frac{\partial}{\partial\theta_{kj}}(\Sigma_{kj}\theta_{kj}h_{j}^{k})+(1-y)\frac{-1}{1-\hat{y}}(1-\hat{y})\hat{y}\frac{\partial}{\partial\theta_{kj}}(\Sigma_{kj}\theta_{kj}h_{j}^{k})$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $=y(1-\hat{y})h_{j}^{k}+(1-y)(-\hat{y})h_{j}^{k}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $=yh_{j}^{k}-y\hat{y}h_{j}^{k}-\hat{y}h_{j}^{k}+y\hat{y}h_{j}^{k}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $=yh_{j}^{k}-\hat{y}h_{j}^{k}=(y-\hat{y})h_{j}^{k}$
\end_inset


\end_layout

\begin_layout Plain Layout
The second term:
\end_layout

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula $\frac{\partial f}{\partial h_{j}^{k}}=y\frac{\partial f}{\partial h_{j}^{k}}log(sigmoid(\Sigma_{kj}\theta_{kj}h_{j}^{k}))+(1-y)\frac{\partial f}{\partial h_{j}^{k}}log(1-sigmoid(\Sigma_{kj}\theta_{kj}h_{j}^{k}))$
\end_inset


\end_layout

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula $\frac{\partial f}{\partial h_{j}^{k}}=y\frac{(1-\hat{y})\hat{y}}{\hat{y}}\frac{\partial f}{\partial h_{j}^{k}}\Sigma_{kj}\theta_{kj}h_{j}^{k}+(1-y)\frac{(\hat{y}-1)\hat{y}}{1-\hat{y}}\frac{\partial f}{\partial h_{j}^{k}}\Sigma_{kj}\theta_{kj}h_{j}^{k}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $ $
\end_inset


\begin_inset Formula $\frac{\partial f}{\partial h_{j}^{k}}=y(1-\hat{y})\theta_{kj}+(1-y)(-\hat{y})\theta_{kj}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $=y\theta_{kj}-\hat{y}y\theta_{kj}-\hat{y}\theta_{kj}+y\hat{y}\theta_{kj}=y\theta_{kj}-\hat{y}\theta_{kj}=(y-\hat{y})\theta_{kj}$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Exercise 2.
\end_layout

\begin_layout Standard
Now derive the followings:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial f}{\partial a_{j}^{k}} & =h_{j}^{k}(1-h_{j}^{k})\frac{\partial f}{\partial h_{j}^{k}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
and
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial f}{\partial v_{s}} & =\sum_{kj}\frac{\partial a_{j}^{k}}{\partial v_{s}}\frac{\partial f}{\partial a_{j}^{k}}=\sum_{k}\left(w^{k}*_{full}\left(\nabla_{a^{k}}f\right)\right)_{s}\\
\frac{\partial f}{\partial w_{i}^{k}} & =\sum_{j}\frac{\partial a_{j}^{k}}{\partial w_{i}^{k}}\frac{\partial f}{\partial a_{j}^{k}}=\left(v*_{filt}\left(\nabla_{a^{k}}f\right)\right)_{i}=\left(v*_{val}\widetilde{\left(\nabla_{a^{k}}f\right)}\right)_{i}\\
\frac{\partial f}{\partial b^{k}} & =\sum_{j}\frac{\partial a_{j}^{k}}{b^{k}}\frac{\partial f}{\partial a_{j}^{k}}=\sum_{j}\frac{\partial f}{\partial a_{j}^{k}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\left(\nabla_{a^{k}}f\right)_{j}\triangleq\frac{\partial f}{\partial a_{j}^{k}}$
\end_inset

.
\end_layout

\begin_layout Standard
Note: since we consider just one hidden layer, there is no need to compute
 
\begin_inset Formula $\frac{\partial f}{\partial v_{s}}$
\end_inset

 in practice.
 However, we derive this since this rule can be useful in deriving backpropagati
on in multiple hidden layer case.
\end_layout

\begin_layout Standard

\series bold
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout

\series bold
Solution:
\end_layout

\begin_layout Plain Layout
First derive: 
\begin_inset Formula $\frac{\partial f}{\partial a_{j}^{k}}=h_{j}^{k}(1-h_{j}^{k})\frac{\partial f}{\partial h_{j}^{k}}$
\end_inset


\end_layout

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula $h_{j}^{k}=sigmoid(a_{j}^{k})$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\frac{\partial h_{j}^{k}}{\partial a_{j}^{k}}=(1-h_{j}^{k})*h_{j}^{k}$
\end_inset


\end_layout

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula $\frac{\partial f}{\partial a_{j}^{k}}=\frac{\partial f}{\partial h_{j}^{k}}\frac{\partial h_{j}^{k}}{\partial a_{j}^{k}}=(1-h_{j}^{k})*h_{j}^{k}\frac{\partial f}{\partial h_{j}^{k}}$
\end_inset


\end_layout

\begin_layout Plain Layout
Next, derive: 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\frac{\partial f}{\partial v_{s}}=\sum_{kj}\frac{\partial a_{j}^{k}}{\partial v_{s}}\frac{\partial f}{\partial a_{j}^{k}}=\sum_{k}\left(w^{k}*_{full}\left(\nabla_{a^{k}}f\right)\right)_{s}$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Generalization
\end_layout

\begin_layout Standard
We will simply assume that there are top-level gradient 
\begin_inset Formula $\frac{\partial}{\partial p_{i}}f$
\end_inset

 where 
\begin_inset Formula $p_{i}$
\end_inset

 are some variables.
 Our strategy is to propagate this towards the layer below using 
\bar under
recursion
\bar default
.
 In general, backpropagation can be viewed as applying 
\begin_inset Quotes eld
\end_inset

chain rule
\begin_inset Quotes erd
\end_inset

 in derivations.
 In convolutional neural networks, we can think about different types of
 layers, and we will generalize over each type of operations.
\end_layout

\begin_layout Subsection
Intermediate layer
\end_layout

\begin_layout Subsubsection
Backpropagating through a linear layer
\end_layout

\begin_layout Itemize
Input of a layer: 
\begin_inset Formula $x$
\end_inset


\end_layout

\begin_layout Itemize
Output of a layer: 
\begin_inset Formula $y_{i}=\sum_{j}\theta_{ij}x_{j}$
\end_inset


\end_layout

\begin_layout Standard
Assume that we are given 
\begin_inset Formula $\frac{\partial f}{\partial y_{i}}$
\end_inset

.
 Then we have the following backpropagation rule:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial f}{\partial x_{j}} & =\sum_{i}\frac{\partial y_{i}}{\partial x_{j}}\frac{\partial f}{\partial y_{i}}\\
 & =\sum_{i}\theta_{ij}\frac{\partial f}{\partial y_{i}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial f}{\partial\theta_{ij}} & =\sum_{i'}\frac{\partial y_{i'}}{\partial\theta_{ij}}\frac{\partial f}{\partial y_{i'}}\\
 & =\sum_{i'}\frac{\partial(\sum_{j'}\theta_{i'j'}x_{j'})}{\partial\theta_{ij}}\frac{\partial f}{\partial y_{i'}}\\
 & =x_{j}\frac{\partial f}{\partial y_{i}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This is the same as backpropagation in a regular neural network.
\end_layout

\begin_layout Subsubsection
Backpropagating through a convolutional layer (valid)
\end_layout

\begin_layout Standard
Now we assume that 
\begin_inset Formula $v$
\end_inset

 has 
\begin_inset Formula $K$
\end_inset

 channels (e.g., for RGB-color image, 
\begin_inset Formula $K=3$
\end_inset

).
 Suppose that 
\begin_inset Formula $\mathbf{a}^{m}=\sum_{k}\mathbf{v}^{k}*_{val}\mathbf{\tilde{w}}^{k,m}=\sum_{k}\mathbf{v}^{k}*_{filt}\mathbf{w}^{k,m}$
\end_inset

.
 For valid convolution, we have 
\begin_inset Formula $a_{i}^{m}=\sum_{k}\sum_{j}v_{i+j-1}^{k}w_{j}^{k,m}$
\end_inset

.
 In other words:
\end_layout

\begin_layout Itemize
Input: 
\begin_inset Formula $v_{j}^{k}$
\end_inset

 
\begin_inset Formula $(k:$
\end_inset

 index for channels, 
\begin_inset Formula $j$
\end_inset

: spatial location), i.e., 
\begin_inset Formula $\mathbf{v}\in R^{K\times N_{V}}$
\end_inset


\end_layout

\begin_layout Itemize
Output: 
\begin_inset Formula $a_{i}^{m}$
\end_inset


\begin_inset Formula $(m:$
\end_inset

 index for feature mapping, 
\begin_inset Formula $i$
\end_inset

: spatial location), i.e., 
\begin_inset Formula $\mathbf{a}\in R^{M\times N_{H}}$
\end_inset

 where 
\begin_inset Formula $N_{H}=N_{V}-N_{W}+1$
\end_inset

.
\end_layout

\begin_layout Itemize
Parameter: 
\begin_inset Formula $w_{j}^{k,m}$
\end_inset

, i.e., 
\begin_inset Formula $\mathbf{w}\in R^{K\times M\times N_{W}}$
\end_inset


\end_layout

\begin_layout Paragraph
Exercise 3.
\end_layout

\begin_layout Standard
Show that 
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial f}{\partial v_{j}^{k}} & = & (\sum_{m}\mathbf{w}^{k,m}*_{full}\nabla_{\mathbf{a}^{m}}f)_{j}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard

\series bold
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout

\series bold
Solution:
\end_layout

\end_inset


\end_layout

\begin_layout Standard
For notational convenience, we often define this as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial f}{\partial v_{j}^{k}} & =reconstruct(\mathbf{w},\nabla_{\mathbf{a}}f)_{j}^{k}\\
 & \triangleq\sum_{m}(\mathbf{w}^{k,m}*_{full}\nabla_{\mathbf{a}^{m}}f)_{j}
\end{align*}

\end_inset


\end_layout

\begin_layout Paragraph
Exercise 4:
\end_layout

\begin_layout Standard
Similarly, show that
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial f}{\partial w_{j}^{k,m}} & = & (\mathbf{v}^{k}*_{valid}\widetilde{\nabla_{\mathbf{a}^{m}}f})_{j}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard

\series bold
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout

\series bold
Solution:
\end_layout

\end_inset


\end_layout

\begin_layout Standard
We can compactly represent this as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial f}{\partial w_{j}^{k,m}} & =outerprod(\mathbf{v}^{k},\nabla_{\mathbf{a}^{m}}f)_{j}\\
 & \triangleq(\mathbf{v}^{k}*_{valid}\nabla_{\mathbf{a}^{m}}\tilde{f})_{j}
\end{align*}

\end_inset


\end_layout

\begin_layout Subsubsection
Backpropagating through a convolutional layer (full)
\end_layout

\begin_layout Standard
Suppose that 
\begin_inset Formula $\mathbf{a}^{m}=\sum_{k}\mathbf{v}^{k}*_{full}\mathbf{w}^{k,m}$
\end_inset

.
 Then by definition of full convolution, we have 
\begin_inset Formula $a_{i}^{m}=\sum_{k}\sum_{j}v_{i-j+1}^{k}w_{j}^{k,m}$
\end_inset

.
 
\end_layout

\begin_layout Paragraph
Exercise 5:
\end_layout

\begin_layout Paragraph

\series medium
Show that:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial f}{\partial v_{j}^{k}} & = & \sum_{m}(\nabla_{\mathbf{a}^{m}}f*_{filt}\mathbf{w}^{k,m})_{j}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout

\series bold
Solution:
\end_layout

\end_inset


\end_layout

\begin_layout Standard
To summarize, we can compactly write as:
\end_layout

\begin_layout Subparagraph
\begin_inset Formula 
\[
\frac{\partial f}{\partial v_{j}^{k}}=inference(\nabla_{\mathbf{a}}f,\mathbf{w})_{j}^{k}=(\sum_{m}\nabla_{\mathbf{a}^{m}}f*_{filt}\mathbf{w}^{k,m})_{j}
\]

\end_inset

Exercise 6:
\end_layout

\begin_layout Standard
Show that
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial f}{\partial w_{j}^{k,m}} & = & ((\nabla_{\mathbf{a}^{m}}f)*_{filt}(\mathbf{v}^{k}))_{j}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout

\series bold
Solution:
\end_layout

\end_inset


\end_layout

\begin_layout Standard
We can write more compactly as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial f}{\partial w_{j}^{k,m}}=outprod(\nabla_{\mathbf{a}^{m}}f,\mathbf{v}^{k})_{j}
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
Remarks: 
\end_layout

\begin_layout Standard
We usually use 
\begin_inset Quotes eld
\end_inset

valid
\begin_inset Quotes erd
\end_inset

 convolution when computing hidden units for classification tasks.
 However, you can use 
\begin_inset Quotes eld
\end_inset

full
\begin_inset Quotes erd
\end_inset

 convolution to reconstruct the original data (e.g., like autoencoder) for
 segmentation tasks.
\end_layout

\begin_layout Subsection
Nonlinearities
\end_layout

\begin_layout Standard
Note that there are no parameters for most nonlinear functions.
 Therefore, we only need to compute the gradient with respect to the hidden
 variables (but not parameters).
 
\end_layout

\begin_layout Standard

\series bold
NOTE: 
\series default
In general, some nonlinearities, such as softmax or sigmoid is often used
 together with the classification loss fuction.
 In such a case, it's more convenient to treat it as a part of classification
 layer (See Section 3.3), instead of treating this as two layers.
 
\end_layout

\begin_layout Subsubsection
Backpropagating through a sigmoid layer
\end_layout

\begin_layout Standard
Suppose that 
\begin_inset Formula $p_{i}=\sigma(x_{i}$
\end_inset

), where 
\begin_inset Formula $\sigma()$
\end_inset

 is a sigmoid function.
 Then the gradient is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial}{\partial x_{i}}f=\frac{\partial p_{i}}{\partial x_{i}}\frac{\partial}{\partial p_{i}}f=\sigma(x_{i})(1-\sigma(x_{i}))\frac{\partial}{\partial p_{i}}f
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Backpropagating through a rectification layer
\end_layout

\begin_layout Standard
For rectification layer, we have 
\begin_inset Formula $p_{i}=|x_{i}|$
\end_inset

.
 Therefore, the backpropagation is simply written as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial}{\partial x_{i}}f=\frac{\partial p_{i}}{\partial x_{i}}\frac{\partial}{\partial p_{i}}f=sign(x_{i})\frac{\partial}{\partial p_{i}}f
\]

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $sign(x)$
\end_inset

 is a subgradient of sign function, so it can be simply defined as matlab
 sign() function.
\end_layout

\begin_layout Subsubsection
Backpropagating through a softmax layer (not for classification)
\end_layout

\begin_layout Standard
We define the softmax layer as: 
\begin_inset Formula $h_{j}=\frac{\exp x_{j}}{\sum_{j'}\exp x_{j'}+1}$
\end_inset

.
 
\end_layout

\begin_layout Paragraph
Exercise 7: 
\end_layout

\begin_layout Standard
Show that the gradient is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial}{\partial x_{j}}f & = & h_{j}\left(\frac{\partial}{\partial h_{j}}f-\sum_{j'}h_{j'}\frac{\partial}{\partial h_{j'}}f\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout

\series bold
Solution:
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Pooling
\end_layout

\begin_layout Standard
Note that there are no parameters for so-called pooling layer.
 Therefore, we only need to compute the gradient with respect to the hidden
 variables (but not parameters).
\end_layout

\begin_layout Subsubsection
Backpropagating through an average pooling layer
\end_layout

\begin_layout Standard
Suppose that 
\begin_inset Formula $p_{i}=\sum_{j\in N_{i}}x_{j}$
\end_inset

, where 
\begin_inset Formula $N_{i}$
\end_inset

 is a local neighbor of node 
\begin_inset Formula $i$
\end_inset

.
 Then the average pooling gradient is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial}{\partial x_{j}}f=\frac{\partial p_{i}}{\partial x_{j}}\frac{\partial}{\partial p_{i}}f=\frac{\partial}{\partial x_{j}}(\sum_{j\in N_{i}}x_{i})\frac{\partial}{\partial p_{i}}f=I(j\in N_{i})\frac{\partial}{\partial p_{i}}f
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Backpropagating through a max-pooling layer
\end_layout

\begin_layout Standard
Suppose that 
\begin_inset Formula $p_{i}=\max_{j\in N_{i}}x_{j}$
\end_inset

, where 
\begin_inset Formula $N_{i}$
\end_inset

 is a local neighbor of node 
\begin_inset Formula $i$
\end_inset

.
 Then the max pooling gradient is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial}{\partial x_{j}}f=\frac{\partial p_{i}}{\partial x_{j}}\frac{\partial}{\partial p_{i}}f=\frac{\partial}{\partial x_{j}}(\max_{j\in N_{i}}x_{j})\frac{\partial}{\partial p_{i}}f=I(j=\arg\max_{j'\in N_{i}}x_{j'})\frac{\partial}{\partial p_{i}}f
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Backpropagating through a softmax-pooling layer
\end_layout

\begin_layout Standard
Suppose that 
\begin_inset Formula $p_{i}=\frac{\sum_{j'\in N_{i}}\exp x_{j'}}{\sum_{j'\in N_{i}}\exp x_{j'}+1}$
\end_inset

, where 
\begin_inset Formula $N_{i}$
\end_inset

 is a local neighbor of node 
\begin_inset Formula $i$
\end_inset

.
 We also define 
\begin_inset Formula $h_{j}=\frac{\exp x_{j}}{\sum_{j'\in N_{i}}\exp x_{j'}+1}$
\end_inset

 (where 
\begin_inset Formula $j\in N_{i})$
\end_inset

.
 
\end_layout

\begin_layout Paragraph

\series bold
Exercise 8:
\end_layout

\begin_layout Standard
Show that the gradient is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial}{\partial x_{j}}f & = & I(j\in N_{i})h_{j}(1-p_{i})\frac{\partial}{\partial p_{i}}f
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout

\series bold
Solution:
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Classification
\end_layout

\begin_layout Subsubsection
Sigmoid output layer (binary classification)
\end_layout

\begin_layout Standard
This is the same as the typical sigmoidal output layer in neural nets.
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\hat{y} & = & sigmoid(\mathbf{\theta}^{T}\mathbf{h})=sigmoid(\sum_{j}\theta_{j}h_{j})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
here, 
\begin_inset Formula $b$
\end_inset

 is a bias for binary classification.
 We define the loss function as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f(y,\hat{y})=y\log\left(sigmoid(\theta^{T}\mathbf{h})\right)+(1-y)\log\left(1-sigm(\theta^{T}\mathbf{h})\right)
\]

\end_inset


\end_layout

\begin_layout Standard
The gradient is as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial f}{\partial\theta_{j}} & = & (y-\hat{y})h_{j}\\
\frac{\partial f}{\partial h_{j}} & = & (y-\hat{y})\theta_{j}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Subsubsection
Softmax output layer (multi-class classification)
\end_layout

\begin_layout Standard
This is the same as the typical sigmoidal output layer in neural nets.
 Consider the following prediction:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\hat{y}^{c} & = & \frac{\exp(\theta^{cT}\mathbf{h})}{\sum_{c'}\exp(\theta^{c'T}\mathbf{h})}=\frac{\exp(\sum_{j}\theta_{j}^{c}h_{j})}{\sum_{c'}\exp(\sum_{j}\theta_{j}^{c'}h_{j})}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The loss function is defined as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f(y,\hat{y})=\sum_{c}I(y=c)\log\left(\hat{y}^{c}\right)
\]

\end_inset


\end_layout

\begin_layout Paragraph
Exercise 9:
\end_layout

\begin_layout Standard
Show that the gradient can be computed as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial f}{\partial\theta_{j}^{c}} & =\left(I(y=c)-\hat{y}^{c}\right)h_{j}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial f}{\partial h_{j}} & =\sum_{c}\left(I(y=c)-\hat{y}^{c}\right)\theta_{j}^{c}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout

\series bold
Solution:
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
NOTE: 
\series default
It's more convenient to treat linear layer and classifier layer together
 as a part of classification layer, instead of treating this as two layers.
 However, the results should be the same.
\end_layout

\begin_layout Subsubsection
Hinge loss layer (multi-class classification with SVM-like loss function)
\end_layout

\begin_layout Standard
This is similar to SVM.
 For binary classification, the loss function is defined as (note that we
 define the label 
\begin_inset Formula $y\in\{1,-1\}$
\end_inset

, not 
\begin_inset Formula $y\in\{0,1\}$
\end_inset

):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f=\max(0,1-y\theta^{T}\mathbf{h})
\]

\end_inset


\end_layout

\begin_layout Paragraph
Exercise 9:
\end_layout

\begin_layout Standard
Show that the gradient can be defined as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial f}{\partial\theta_{j}} & =-I(y\theta^{T}\mathbf{h}\le1)yh_{j}\\
\frac{\partial f}{\partial h_{j}} & =-I(y\theta^{T}\mathbf{h}\le1)y\theta_{j}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout

\series bold
Solution:
\end_layout

\end_inset


\end_layout

\begin_layout Standard
For multi-class classification, we also define the loss function as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f=\max(0,1-\theta^{yT}\mathbf{h}+\max_{c\neq y}\theta^{cT}\mathbf{h})
\]

\end_inset


\end_layout

\begin_layout Paragraph
Exercise 10:
\end_layout

\begin_layout Standard
Show that the gradient can be computed as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial f}{\partial\theta_{j}^{c}} & =I(\theta^{yT}\mathbf{h}-\max_{c\neq y}\theta^{cT}\mathbf{h}\le1)\left(-I(c=y)h_{j}+I(c\neq y\cap c=\arg\max_{c'\neq y}\theta^{c'}\mathbf{h})h_{j}\right)\\
 & =I(\theta^{yT}\mathbf{h}-\max_{c\neq y}\theta^{cT}\mathbf{h}\le1)\left(-I(c=y)h_{j}+I(c\neq c*)h_{j}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
and 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial f}{\partial h_{j}} & =I(\theta^{yT}\mathbf{h}-\max_{c\neq y}\theta^{cT}\mathbf{h}\le1)\left(-\theta_{j}^{y}+\theta_{j}^{c*}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $c*=\arg\max_{c\neq y}\theta^{c}\mathbf{h}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout

\series bold
Solution:
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
NOTE
\series default
: So far, we didn't include regularization of weights (such as L2 regularization
 
\begin_inset Formula $\frac{\lambda}{2}\mathbf{w}^{T}\mathbf{w}$
\end_inset

) in the objective function, mainly to avoid clutter in exercise.
 However, it is important to add regularization and cross validate the regulariz
ation parameter.
 Neural networks are typically known to have very large capacity, and without
 regularization it may suffer from overfitting.
\end_layout

\end_body
\end_document
